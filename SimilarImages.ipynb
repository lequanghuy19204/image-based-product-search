{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook represents a prototypical python 3 implementation for a without safeguards, without asserts, with fixed paramterized model. For larger data sets it is advised to store intermediate results, e.g. as pickle files.\n",
    "\n",
    "Theory https://towardsdatascience.com/effortlessly-recommending-similar-images-b65aff6aabfb\n",
    "\n",
    "\n",
    "\n",
    "# Rescaling\n",
    "\n",
    "We assume to have a folder \"originalImages\" in the working directory. It shall contain jpg images.\n",
    "As we will employ resnet18 using PyTorch, we need to resize the images to normalized 224x224 images\n",
    "In a first step they are resized and stored in a different folder inputImagesCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms \n",
    "\n",
    "# Kích thước cần thiết cho CNN\n",
    "inputDim = (224,224)\n",
    "# Thư mục chứa ảnh gốc\n",
    "inputDir = \"originalImages\"\n",
    "# Thư mục chứa ảnh đã được chuyển đổi\n",
    "inputDirCNN = \"scaled\"\n",
    "\n",
    "# Tạo thư mục chứa ảnh đã được chuyển đổi nếu chưa tồn tại\n",
    "os.makedirs(inputDirCNN, exist_ok = True)\n",
    "\n",
    "# Chuẩn bị quy trình chuyển đổi ảnh\n",
    "transformationForCNNInput = transforms.Compose([transforms.Resize(inputDim)])\n",
    "\n",
    "# Duyệt qua tất cả ảnh trong thư mục gốc\n",
    "for imageName in os.listdir(inputDir):    \n",
    "    # Mở ảnh\n",
    "    img = Image.open(os.path.join(inputDir, imageName))\n",
    "    # Chuyển đổi ảnh theo quy trình đã chuẩn bị\n",
    "    newImg = transformationForCNNInput(img)\n",
    "\n",
    "    # Sao chép thông tin xoay ảnh từ ảnh gốc và lưu, nếu không ảnh chuyển đổi có thể bị xoay\n",
    "    if('exif' in img.info):\n",
    "        exif = img.info['exif']\n",
    "        newImg.save(os.path.join(inputDirCNN, imageName), exif=exif)\n",
    "    else:\n",
    "        newImg.save(os.path.join(inputDirCNN, imageName))\n",
    "    \n",
    "    # Đóng ảnh\n",
    "    newImg.close()\n",
    "    img.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the similarity matrix with Resnet18\n",
    "\n",
    "Let us first calculate the feature vectors with resnet18 on a CPU. The input is normalized to the ImageNet mean values/standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "\n",
    "# for this prototype we use no gpu, cuda= False and as model resnet18 to obtain feature vectors\n",
    "\n",
    "class Img2VecResnet18():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.numberFeatures = 512\n",
    "        self.modelName = \"resnet-18\"\n",
    "        self.model, self.featureLayer = self.getFeatureLayer()\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        \n",
    "        # normalize the resized images as expected by resnet18\n",
    "        # [0.485, 0.456, 0.406] --> normalized mean value of ImageNet, [0.229, 0.224, 0.225] std of ImageNet\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    def getVec(self, img):\n",
    "        image = self.normalize(self.toTensor(img)).unsqueeze(0).to(self.device)\n",
    "        embedding = torch.zeros(1, self.numberFeatures, 1, 1)\n",
    "\n",
    "        def copyData(m, i, o): embedding.copy_(o.data)\n",
    "\n",
    "        h = self.featureLayer.register_forward_hook(copyData)\n",
    "        self.model(image)\n",
    "        h.remove()\n",
    "\n",
    "        return embedding.numpy()[0, :, 0, 0]\n",
    "\n",
    "    def getFeatureLayer(self):\n",
    "        \n",
    "        cnnModel = models.resnet18(pretrained=True)\n",
    "        layer = cnnModel._modules.get('avgpool')\n",
    "        self.layer_output_size = 512\n",
    "        \n",
    "        return cnnModel, layer\n",
    "        \n",
    "\n",
    "# generate vectors for all the images in the set\n",
    "img2vec = Img2VecResnet18() \n",
    "\n",
    "allVectors = {}\n",
    "print(\"Converting images to feature vectors:\")\n",
    "for image in tqdm(os.listdir(\"scaled\")):\n",
    "    img = Image.open(os.path.join(\"scaled\", image))\n",
    "    # Convert image to RGB if it's not already in RGB\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    vec = img2vec.getVec(img)\n",
    "    allVectors[image] = vec\n",
    "    img.close() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "Calculate for all vectors the cosine similarity to the other vectors.\n",
    "Note that this matrix may become huge, hence infefficient, with many thousands of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let us define a function that calculates the cosine similarity entries in the similarity matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getSimilarityMatrix(vectors):\n",
    "    v = np.array(list(vectors.values())).T\n",
    "    sim = np.inner(v.T, v.T) / ((np.linalg.norm(v, axis=0).reshape(-1,1)) * ((np.linalg.norm(v, axis=0).reshape(-1,1)).T))\n",
    "    keys = list(vectors.keys())\n",
    "    matrix = pd.DataFrame(sim, columns = keys, index = keys)\n",
    "    \n",
    "    return matrix\n",
    "        \n",
    "similarityMatrix = getSimilarityMatrix(allVectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare top-k lists\n",
    "Now that the similarity matrix is fully available, the last step is to sort the values per item and store the top similar entries in another data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "import pickle\n",
    "\n",
    "k = 5 # the number of top similar images to be stored\n",
    "\n",
    "similarNames = pd.DataFrame(index = similarityMatrix.index, columns = range(k))\n",
    "similarValues = pd.DataFrame(index = similarityMatrix.index, columns = range(k))\n",
    "\n",
    "for j in tqdm(range(similarityMatrix.shape[0])):\n",
    "    kSimilar = similarityMatrix.iloc[j, :].sort_values(ascending = False).head(k)\n",
    "    similarNames.iloc[j, :] = list(kSimilar.index)\n",
    "    similarValues.iloc[j, :] = kSimilar.values\n",
    "    \n",
    "similarNames.to_pickle(\"similarNames.pkl\")\n",
    "similarValues.to_pickle(\"similarValues.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and visualize similar images for four example inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# take three examples from the provided image set and plot\n",
    "inputImages = [\"462551404_8354429251321777_1590794617779575944_n.png\"]\n",
    "\n",
    "numCol = 5\n",
    "numRow = 1\n",
    "\n",
    "def setAxes(ax, image, query = False, **kwargs):\n",
    "    fontsize = 8\n",
    "    value = kwargs.get(\"value\", None)\n",
    "    if query:\n",
    "        ax.set_xlabel(\"Input\\n{0}\".format(image), fontsize = fontsize)\n",
    "    else:\n",
    "        ax.set_xlabel(\"Similar value {1:1.3f}\\n{0}\".format( image,  value), fontsize = fontsize)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "def FindSimilarImages(imgPath):\n",
    "    if imgPath in set(similarNames.index):\n",
    "        imgPaths = list(similarNames.loc[imgPath, :])\n",
    "        vals = list(similarValues.loc[imgPath, :])\n",
    "\n",
    "        if imgPath in imgPaths:\n",
    "            assert_almost_equal(max(vals), 1, decimal = 5)\n",
    "            imgPaths.remove(imgPath)\n",
    "            vals.remove(max(vals))\n",
    "\n",
    "        return imgPaths, vals\n",
    "    else:\n",
    "        print(\"'{}' Unknown image\".format(imgPath))\n",
    "        \n",
    "def plotSimilarImages(imgPath):\n",
    "    foundImgPaths, foundValues = FindSimilarImages(imgPath)\n",
    "    fig = plt.figure(figsize=(10, 20))\n",
    "    \n",
    "    # Thêm ảnh tìm kiếm vào danh sách ảnh giống\n",
    "    imgPaths = [imgPath] + foundImgPaths\n",
    "    vals = [1] + foundValues  # Giá trị tương tự cho ảnh tìm kiếm là 1\n",
    "    # now plot the  most simliar images\n",
    "    for j in range(0, numCol*numRow):\n",
    "        ax = []\n",
    "        img = Image.open(os.path.join(inputDir, imgPaths[j]))\n",
    "        ax.append(fig.add_subplot(numRow, numCol, j+1))\n",
    "        setAxes(ax[-1], imgPaths[j], value = vals[j])\n",
    "        img = img.convert('RGB')\n",
    "        plt.imshow(img)\n",
    "        img.close()\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "for imgPath in inputImages:\n",
    "    plotSimilarImages(imgPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
